#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ™ºèƒ½æ€»ç»“å™¨ - å¯¹æ£€ç´¢å†…å®¹è¿›è¡Œæ€»ç»“
"""

import re
from typing import List, Dict

class ContentSummarizer:
    def __init__(self, config: Dict):
        self.config = config
        self.max_summary_length = config.get('max_summary_length', 600)
    
    def summarize_search_results(self, query: str, search_results: List[Dict]) -> str:
        """å¯¹æœç´¢ç»“æœè¿›è¡Œæ™ºèƒ½æ€»ç»“"""
        if not search_results:
            return f"æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ä¸ã€Œ{query}ã€ç›¸å…³çš„å†…å®¹ã€‚"
        
        # 1. æå–å…³é”®ä¿¡æ¯
        key_info = self._extract_key_information(query, search_results)
        
        # 2. ç”Ÿæˆæ€»ç»“
        summary = self._generate_summary(query, key_info)
        
        return summary
    
    def _extract_key_information(self, query: str, results: List[Dict]) -> Dict:
        """æå–å…³é”®ä¿¡æ¯"""
        query_keywords = set(re.findall(r'[\w\u4e00-\u9fff]+', query.lower()))
        
        key_info = {
            'main_points': [],
            'relevant_content': [],
            'sources': set(),
            'keywords_found': set()
        }
        
        for result in results:
            content = result.get('content', '')
            source = result.get('source', 'æœªçŸ¥æ¥æº')
            score = result.get('score', 0)
            
            # æå–ç›¸å…³å¥å­
            sentences = self._split_sentences(content)
            relevant_sentences = []
            
            for sentence in sentences:
                sentence_words = set(re.findall(r'[\w\u4e00-\u9fff]+', sentence.lower()))
                # è®¡ç®—å¥å­ä¸æŸ¥è¯¢çš„ç›¸å…³æ€§
                relevance = len(query_keywords & sentence_words) / len(query_keywords) if query_keywords else 0
                
                if relevance > 0.1 or score > 0.5:  # ç›¸å…³æ€§é˜ˆå€¼
                    relevant_sentences.append({
                        'text': sentence.strip(),
                        'relevance': relevance,
                        'source': source
                    })
                    key_info['keywords_found'].update(query_keywords & sentence_words)
            
            # æŒ‰ç›¸å…³æ€§æ’åºå¹¶å–å‰å‡ å¥
            relevant_sentences.sort(key=lambda x: x['relevance'], reverse=True)
            key_info['relevant_content'].extend(relevant_sentences[:2])
            key_info['sources'].add(source)
        
        # å»é‡å¹¶æ’åº
        key_info['relevant_content'] = sorted(
            key_info['relevant_content'], 
            key=lambda x: x['relevance'], 
            reverse=True
        )[:5]  # æœ€å¤š5ä¸ªå…³é”®å¥å­
        
        return key_info
    
    def _split_sentences(self, text: str) -> List[str]:
        """åˆ†å‰²å¥å­"""
        # ä¸­è‹±æ–‡å¥å­åˆ†å‰²
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿï¼›\.\!\?\;]', text)
        return [s.strip() for s in sentences if len(s.strip()) > 10]
    
    def _generate_summary(self, query: str, key_info: Dict) -> str:
        """ç”Ÿæˆæ€»ç»“"""
        relevant_content = key_info['relevant_content']
        sources = key_info['sources']
        keywords_found = key_info['keywords_found']
        
        if not relevant_content:
            return f"æ‰¾åˆ°äº†ç›¸å…³æ–‡æ¡£ï¼Œä½†æ²¡æœ‰å‘ç°ä¸ã€Œ{query}ã€ç›´æ¥ç›¸å…³çš„å…·ä½“å†…å®¹ã€‚"
        
        # æ„å»ºæ€»ç»“
        summary_parts = []
        
        # å¼€å¤´
        if keywords_found:
            found_keywords = 'ã€'.join(list(keywords_found)[:3])
            summary_parts.append(f"å…³äºã€Œ{query}ã€ï¼Œæ‰¾åˆ°ä»¥ä¸‹ç›¸å…³ä¿¡æ¯ï¼š")
        else:
            summary_parts.append(f"æ ¹æ®æ£€ç´¢ç»“æœï¼Œå…³äºã€Œ{query}ã€çš„ä¿¡æ¯å¦‚ä¸‹ï¼š")
        
        # ä¸»è¦å†…å®¹
        main_content = []
        for i, item in enumerate(relevant_content[:3], 1):
            content = item['text']
            # é™åˆ¶æ¯ä¸ªè¦ç‚¹çš„é•¿åº¦
            if len(content) > 200:
                content = content[:200] + "..."
            main_content.append(f"{i}. {content}")
        
        summary_parts.extend(main_content)
        
        # æ¥æºä¿¡æ¯
        if sources:
            source_list = list(sources)[:6]  # æœ€å¤šæ˜¾ç¤º6ä¸ªæ¥æº
            if len(source_list) == 1:
                summary_parts.append(f"\nğŸ“„ æ¥æºï¼š{source_list[0]}")
            else:
                sources_text = "ã€".join(source_list)
                summary_parts.append(f"\nğŸ“„ æ¥æºï¼š{sources_text}")
        
        # ç»„åˆæ€»ç»“
        full_summary = "\n\n".join(summary_parts)
        
        # é•¿åº¦æ§åˆ¶
        if len(full_summary) > self.max_summary_length:
            full_summary = full_summary[:self.max_summary_length] + "..."
        
        return full_summary
    
    def _extract_key_phrases(self, text: str, query: str) -> List[str]:
        """æå–å…³é”®çŸ­è¯­"""
        query_words = set(re.findall(r'[\w\u4e00-\u9fff]+', query.lower()))
        
        # ç®€å•çš„å…³é”®çŸ­è¯­æå–
        phrases = []
        sentences = self._split_sentences(text)
        
        for sentence in sentences:
            sentence_words = set(re.findall(r'[\w\u4e00-\u9fff]+', sentence.lower()))
            if query_words & sentence_words:  # åŒ…å«æŸ¥è¯¢è¯çš„å¥å­
                # æå–åŒ…å«æŸ¥è¯¢è¯çš„çŸ­è¯­
                words = re.findall(r'[\w\u4e00-\u9fff]+', sentence)
                for i, word in enumerate(words):
                    if word.lower() in query_words:
                        # æå–å‰åå„2ä¸ªè¯ä½œä¸ºçŸ­è¯­
                        start = max(0, i-2)
                        end = min(len(words), i+3)
                        phrase = ''.join(words[start:end])
                        if len(phrase) > 4:
                            phrases.append(phrase)
        
        return phrases[:5]  # è¿”å›å‰5ä¸ªå…³é”®çŸ­è¯­